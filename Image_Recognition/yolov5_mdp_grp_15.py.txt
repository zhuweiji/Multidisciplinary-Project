# -*- coding: utf-8 -*-
"""YOLOv5_MDP_GRP_15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16_caegX4D-M0hng1-KI8HRlSVI9HPCTI

### Get and  dependencies for setting up YOLOv5
"""

# Get YOLOv5 file from this repo
!git clone https://github.com/ultralytics/yolov5

# Install neccessary libararies
!pip install -r /content/yolov5/requirements.txt

# Install neccesary libararies
import torch   # YOLOv5 works with torch rather than tensorflow
from IPython.display import Image  # for displaying images
import os 
import random
import shutil
from sklearn.model_selection import train_test_split
import xml.etree.ElementTree as ET
from xml.dom import minidom
from tqdm import tqdm
from PIL import Image, ImageDraw
import numpy as np
import matplotlib.pyplot as plt

random.seed(108)

"""### TOY EXAMPLE ( WOULD NEED TO CHANGE SOME THINGS HERE TO MATCH OUR DATA WHEN WE START TO TAKE PROPER PICS )"""

# Commented out IPython magic to ensure Python compatibility.
# Define the datapath ( We would change this to /content/mdp_symbolRecog )

# %mkdir /content/MDP_IMGS

# Download the roadsign data (CAN DELETE/IGNORE THIS CELL LATER, IM JUST USING IT TO TEST THINGS OUT AND SEE THE FORMAT OF HOW THINGS ARE DONE )
#!wget -O RoadSignDetectionDataset.zip "https://arcraftimages.s3-accelerate.amazonaws.com/Datasets/RoadSigns/RoadSignsPascalVOC.zip?region=us-east-2"


# mount  google drive
from google.colab import drive
drive.mount('/content/drive')

# The road dataset is a small one, containing only 877 images in total
#!unzip /content/RoadSignDetectionDataset.zip
!unzip /content/drive/MyDrive/DONE.zip

"""#### In this example the data wasnt in the required format for YOLOv5
ie. YOLOv5 needs annotations in txt format, for this dataset we do some conversions from .xml to txt, 

BUT we can skip this step for our model that we'll be creating, as we can just use ROBOFLOW to create the annotations and load them in here ( ROBOFLOW NEEDS US TO DO 1000 IMGS AT A TIME - limits of a free account )

Alternatively, we can download this application to do the annotations for us which gives it to us in the required format ( https://github.com/ivangrov/ModifiedOpenLabelling ) 









YOLOv5 FORMAT
- Each row is class x_center y_center width height format.
- Box coordinates must be normalized by the dimensions of the image (i.e. have values between 0 and 1)
- Class numbers are zero-indexed (start from 0).

###### Define the classes we have ( in our case 31 classes/symbols ) in a dictionary
"""

# Dictionary that maps class names to IDs  # CHANGE THIS TO OUR CLASSES WHEN WE PUT IN OUT DATA 
# #class_name_to_id_mapping = {"trafficlight": 0,
#                            "stop": 1,
#                            "speedlimit": 2,
#                            "crosswalk": 3}

# Classficaion dictionary
class_name_to_id_mapping = {'1':0, '2':1, '3':2, '4':3, '5':4, '6':5, '7':6, 
'8':7, '9':8, 'A':9, 'B':10, 'C':11, 'Circle':12, 'D':13, 'Down':14, 'E':15, 'F':16, 
'G':17, 'H':18, 'Left':19, 'Right':20, 'S':21, 'T':22, 'Target':23, 
'U':24, 'Up':25, 'V':26, 'W':27, 'X':28, 'Y':29, 'Z':30}

from matplotlib.cbook import index_of
# Get the annotations
# annotations = [os.path.join('/content/DONE/1_annotated/images', x) for x in os.listdir('/content/DONE/1_annotated/images') if x[-3:] == "jpg"]
# annotations

# # # Convert and save the annotations
# # for ann in tqdm(annotations):
# #     info_dict = extract_info_from_xml(ann)
# #     convert_to_yolov5(info_dict)
# annotations = [os.path.join('/content/DONE/1_annotated/labels', x) for x in os.listdir('/content/DONE/1_annotated/labels') if x[-3:] == "txt"]
# annotations

pic_limit = 350   # Defines how many imgs per class. NEED TO SET ACCORDING TO RAM

class_list_path = ['/content/DONE/1_annotated', '/content/DONE/2_annotated', '/content/DONE/3_annotated', '/content/DONE/4_annotated', '/content/DONE/5_annotated',
                   '/content/DONE/6_annotated','/content/DONE/7_annotated','/content/DONE/8_annotated','/content/DONE/9_annotated','/content/DONE/A_annotated',
                   '/content/DONE/B_annotated','/content/DONE/C_annotated','/content/DONE/Circle_annotated','/content/DONE/D_annotated','/content/DONE/Down',
                   '/content/DONE/E_annotated','/content/DONE/F_annotated','/content/DONE/G_annotated','/content/DONE/H_annotated','/content/DONE/Left_annotated',
                   '/content/DONE/Right_annotated','/content/DONE/S_annotated','/content/DONE/T_annotated','/content/DONE/Target_annotated','/content/DONE/U_annotated',
                   '/content/DONE/Up_annotated','/content/DONE/V_annotated','/content/DONE/W_annotated','/content/DONE/X_annotated','/content/DONE/Y_annotated',
                   '/content/DONE/Z_annotated']

class_list_path_images = ['/content/DONE/1_annotated/images', '/content/DONE/2_annotated/images', '/content/DONE/3_annotated/images', '/content/DONE/4_annotated/images', '/content/DONE/5_annotated/images',
                   '/content/DONE/6_annotated/images','/content/DONE/7_annotated/images','/content/DONE/8_annotated/images','/content/DONE/9_annotated/images','/content/DONE/A_annotated/images',
                   '/content/DONE/B_annotated/images','/content/DONE/C_annotated/images','/content/DONE/Circle_annotated/images','/content/DONE/D_annotated/images','/content/DONE/Down/images',
                   '/content/DONE/E_annotated/images','/content/DONE/F_annotated/images','/content/DONE/G_annotated/images','/content/DONE/H_annotated/images','/content/DONE/Left_annotated/images',
                   '/content/DONE/Right_annotated/images','/content/DONE/S_annotated/images','/content/DONE/T_annotated/images','/content/DONE/Target_annotated/images','/content/DONE/U_annotated/images',
                   '/content/DONE/Up_annotated/images','/content/DONE/V_annotated/images','/content/DONE/W_annotated/images','/content/DONE/X_annotated/images','/content/DONE/Y_annotated/images',
                   '/content/DONE/Z_annotated/images']

class_list_path_labels = ['/content/DONE/1_annotated/labels', '/content/DONE/2_annotated/labels', '/content/DONE/3_annotated/labels', '/content/DONE/4_annotated/labels', '/content/DONE/5_annotated/labels',
                   '/content/DONE/6_annotated/labels','/content/DONE/7_annotated/labels','/content/DONE/8_annotated/labels','/content/DONE/9_annotated/labels','/content/DONE/A_annotated/labels',
                   '/content/DONE/B_annotated/labels','/content/DONE/C_annotated/labels','/content/DONE/Circle_annotated/labels','/content/DONE/D_annotated/labels','/content/DONE/Down/labels',
                   '/content/DONE/E_annotated/labels','/content/DONE/F_annotated/labels','/content/DONE/G_annotated/labels','/content/DONE/H_annotated/labels','/content/DONE/Left_annotated/labels',
                   '/content/DONE/Right_annotated/labels','/content/DONE/S_annotated/labels','/content/DONE/T_annotated/labels','/content/DONE/Target_annotated/labels','/content/DONE/U_annotated/labels',
                   '/content/DONE/Up_annotated/labels','/content/DONE/V_annotated/labels','/content/DONE/W_annotated/labels','/content/DONE/X_annotated/labels','/content/DONE/Y_annotated/labels',
                   '/content/DONE/Z_annotated/labels']

print(len(class_list_path), len(class_list_path_images), len(class_list_path_labels))



annotations = []
count = 0
for i in class_list_path_images:
  jpg_path = [os.path.join(i, x) for x in os.listdir(i) if x[-3:] == "jpg"]
  random.shuffle(jpg_path)
  jpg_path = jpg_path[0:pic_limit]

  for x in jpg_path:
    path = x
    index_of_images_text = path.find("images")
    index_of_images_text = index_of_images_text + 7
    path = path[index_of_images_text:]
    path = path[:-3] + "txt"
    path = os.path.join(class_list_path_labels[count], path)
    annotations.append(path)
  count = count + 1
    
print(annotations, len(annotations))

# WHAT WE DO NEED TO DO IS DEFINE AN ARRAY ANNOATIONS WITH ALL THE .TXT FILE PATHS
#annotations = [PUT ALL THE .TXT FILE PATHS IN HERE, WE CAN PRINT THE ANNOATIONS ARRAY AS IN THE EXAMPLE AS A REFERENCE OF HOW IT SHOULD LOOK LIKE]

"""##### Display the imgs with bounding boxes overlaying them as a sanity check ( SHOULD RUN THIS ON OUR DATASET TOO AS A SANITY CHECK )"""

# DISPLAY AN IMG TO SEE THE BOUNDING BOXES 
# WE SHOULD DO THIS TOO

random.seed(63)

class_id_to_name_mapping = dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys()))

def plot_bounding_box(image, annotation_list):
    annotations = np.array(annotation_list)
    w, h = image.size
    
    plotted_image = ImageDraw.Draw(image)

    transformed_annotations = np.copy(annotations)
    print(transformed_annotations)
    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w
    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h 
    
    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)
    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)
    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]
    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]
    
    for ann in transformed_annotations:
        obj_cls, x0, y0, x1, y1 = ann
        plotted_image.rectangle(((x0,y0), (x1,y1)))
        
        plotted_image.text((x0, y0 - 10), class_id_to_name_mapping[(int(obj_cls))])
    
    plt.imshow(np.array(image))
    plt.show()

MAX = 5
count = 0
for i in annotations:
  # Get any random annotation file 
  annotation_file = i
  print(annotation_file)
  with open(annotation_file, "r") as file:
      FileContent = file.read()
      print(FileContent.split("\n"), "BYE")
      annotation_list = FileContent.split("\n")

      annotation_list = [x.split(" ") for x in annotation_list]
      print(annotation_list ,"HI")
      annotation_list = [[float(y) for y in x ] for x in annotation_list]
  print(annotation_list ,"HI")
  #Get the corresponding image file
  annotation_file
  image_file = annotation_file.replace("labels", "images").replace("txt", "jpg")  # THE EXAMPLE USED >PNG OUR WOULD BE JPG< SO JUST CHANGE PNG HERE TO JPG


  assert os.path.exists(image_file)

  #Load the image
  image = Image.open(image_file)
  print(image, annotation_list)
  #Plot the Bounding Box
  plot_bounding_box(image, annotation_list)
  if (count > MAX):
    break
  count = count + 1

"""### Split the Dataset into train, validation , and test sets 

Next we partition the dataset into

 train, 80% of the data

 validation, 10% of the data

 and test sets 10% of the data

You can change the split values according to your convenience. ( THIS RATIO SHOULD BE OK I THINK?)
"""

# Read images and annotations
image_file = []
for i in annotations:
  path = i.replace("labels", "images").replace("txt", "jpg") 
  image_file.append(path)
images = image_file
#print(annotations)

# SANITY CHECK TO MAKE SURE PATHS ARE CORRECT
MAX = 5
count = 0
for i in images:
    #Load the image
  image = Image.open(i)
  plt.imshow(np.array(image))
  plt.show()
  if (count > MAX):
    break
  count = count + 1
print(images)

# # CHANGE THIS TO WHAT WE NEED LATER ON WHEN WE PUT IN OUR OWN DATA
# image_path = "/content/images"
# annotation_path = "/content/annotations"

# images = [os.path.join(image_path, x) for x in os.listdir(image_path)]
# annotations = [os.path.join(annotation_path, x) for x in os.listdir(annotation_path) if x[-3:] == "txt"]

# images.sort()
# annotations.sort()

# # Split the dataset into train-valid-test splits 
train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)
val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)

# Create seperate locations to store the data so that its more organized
#!mkdir images/train images/val images/test annotations/train annotations/val annotations/test


os.makedirs("/content/images/train")
os.makedirs("/content/images/val")
os.makedirs("/content/images/test")
os.makedirs("/content/annotations/train")
os.makedirs("/content/annotations/val")
os.makedirs("/content/annotations/test")

# Move the files to their respective folders.

#Utility function to move images 
def move_files_to_folder(list_of_files, destination_folder):
    for f in list_of_files:
        try:
            shutil.move(f, destination_folder)
        except:
            print(f)
            assert False

# Move the splits into their folders
move_files_to_folder(train_images, '/content/images/train')
move_files_to_folder(val_images, '/content/images/val/')
move_files_to_folder(test_images, '/content/images/test')

move_files_to_folder(train_annotations, '/content/annotations/train')
move_files_to_folder(val_annotations, '/content/annotations/val')
move_files_to_folder(test_annotations, '/content/annotations/test')

# Rename the annotations folder to labels, as this is where YOLO v5 expects the annotations to be located in.

"""#### Setup the YAML file


Data Config File

Details for the dataset you want to train your model on are defined by the data config YAML file. The following parameters have to be defined in a data config file:

train, test, and val: Locations of train, test, and validation images.

nc: Number of classes in the dataset.

names: Names of the classes in the dataset. The index of the classes in this list would be used as an identifier for the class names in the code.

Create a new file called mdpImgRecognition.yaml and place it in the yolov5/data folder. Then populate it with the following.



train: /content/images/train/

val: /content/images/val/

test: /content/images/test/

###### number of classes
nc: 4

###### class names
names: ["trafficlight","stop", "speedlimit","crosswalk"]


YOLO v5 expects to find the training labels for the images in the folder whose name can be derived by replacing images with labels in the path to dataset images. For example, in the example above, YOLO v5 will look for train labels in ../content/labels/train/.

#### Hyperparameter Config File
The hyperparameter config file helps us define the hyperparameters for our neural network. We are going to use the default one, data/hyp.scratch.yaml. This is what it looks like.
"""

# Hyperparameters for COCO training from scratch
# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300
# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials


lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)
lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum/Adam beta1
weight_decay: 0.0005  # optimizer weight decay 5e-4
warmup_epochs: 3.0  # warmup epochs (fractions ok)
warmup_momentum: 0.8  # warmup initial momentum
warmup_bias_lr: 0.1  # warmup initial bias lr
box: 0.05  # box loss gain
cls: 0.5  # cls loss gain
cls_pw: 1.0  # cls BCELoss positive_weight
obj: 1.0  # obj loss gain (scale with pixels)
obj_pw: 1.0  # obj BCELoss positive_weight
iou_t: 0.20  # IoU training threshold
anchor_t: 4.0  # anchor-multiple threshold
# anchors: 3  # anchors per output layer (0 to ignore)
fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)
hsv_h: 0.015  # image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # image HSV-Value augmentation (fraction)
degrees: 0.0  # image rotation (+/- deg)
translate: 0.1  # image translation (+/- fraction)
scale: 0.5  # image scale (+/- gain)
shear: 0.0  # image shear (+/- deg)
perspective: 0.0  # image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # image flip up-down (probability)
fliplr: 0.5  # image flip left-right (probability)
mosaic: 1.0  # image mosaic (probability)
mixup: 0.0  # image mixup (probability)

# You can edit this file, save a new file, and specify it as an argument to the train script.

"""##### Custom Network Architecture [ I DONT THINK WE NEED THIS, SO CAN SKIP THIS STEP ]
YOLO v5 also allows you to define your own custom architecture and anchors if one of the pre-defined networks doesn't fit the bill for you. For this you will have to define a custom weights config file. For this example, we use the the yolov5s.yaml. This is what it looks like.
"""

# I DONT THINK WE NEED THIS, SO CAN SKIP THIS STEP

# parameters
nc: 80  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple

# anchors
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Focus, [64, 3]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 9, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 1, SPP, [1024, [5, 9, 13]]],
   [-1, 3, C3, [1024, False]],  # 9
  ]

# YOLOv5 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)

   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]

!python "/content/yolov5/train.py" --img 640 --cfg /content/yolov5/models/yolov5s.yaml --hyp /content/yolov5/data/hyps/hyp.scratch.yaml --batch 32 --epochs 70 --data /content/yolov5/data/mdpImgRecognition.yaml --weights yolov5s.pt --workers 24 --name yolo_mdp_det

"""#### Inference ( PREDICTION )
There are many ways to run inference using the detect.py file.

The source flag defines the source of our detector, which can be:

A single image

A folder of images

Video

Webcam

...and various other formats. We want to run it over our test images so we set the source flag to /content/Road_Sign_Dataset/images/test


The weights flag defines the path of the model which we want to run our detector with.

conf flag is the thresholding objectness confidence.

name flag defines where the detections are stored. We set this flag to 
yolo_road_det; therefore, the detections would be stored in runs/detect/yolo_road_det/.


"""

!cp -rx "/content/yolov5" "/content/drive/My Drive"

!python /content/yolov5/detect.py --source /content/images/test --weights /content/yolov5/runs/train/yolo_mdp_det2/weights/best.pt --conf 0.25 --name yolo_road_det
